{"Page 1": {"response": "<h3>\n Patent Application Summary: US 2020/0402247 A1\n</h3>\n<p>\n<strong>\n  Publication Information:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Patent Number:\n  </strong>\n  US 2020/0402247 A1\n </li>\n<li>\n<strong>\n   Publication Date:\n  </strong>\n  December 24, 2020\n </li>\n<li>\n<strong>\n   Applicant:\n  </strong>\n  Lightform, Inc., San Francisco, CA, US\n </li>\n<li>\n<strong>\n   Inventors:\n  </strong>\n  Kevin Karsch, Rajinder Sodhi, Brett Jones, Pulkit Budhiraja, Phil Reyneri, Douglas Rieck, Andrew Kilkenny, Ehsan Noursalehi, Derek Nedelman, Laura LaPerche, Brittany Factura, all from San Francisco, CA, US.\n </li>\n</ul>\n<p>\n<strong>\n  Classification:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   International Classification:\n  </strong>\n  G06T 7/521, G06T 7/536, G06K 9/32, G06K 9/46, GOIB 11/25\n </li>\n<li>\n<strong>\n   CPC Classification:\n  </strong>\n  G06T 7/521, G06T 7/536, G06K 9/3241, GO6T 19/006, GOIB 11/2513, GOIB 11/2536, G06K 9/4652\n </li>\n</ul>\n<p>\n<strong>\n  Application Details:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Application Number:\n  </strong>\n  16/872,237\n </li>\n<li>\n<strong>\n   Filing Date:\n  </strong>\n  May 11, 2020\n </li>\n<li>\n<strong>\n   Related Applications:\n  </strong>\n  Continuation of application No. 16/447,792 (now Pat. No. 10,692,233) and application No. 16/146,679 (now Pat. No. 10,373,325).\n </li>\n</ul>\n<p>\n<strong>\n  Abstract:\n </strong>\n The method involves augmenting a real scene with projected visual content through the following steps:\n</p>\n<ol>\n<li>\n  Setup frames are served to a projector directed at the scene.\n </li>\n<li>\n  A peripheral control module with a camera captures images while the projector displays the setup frames and a baseline image of the scene.\n </li>\n<li>\n  A pixel correspondence map is calculated from the captured images and setup frames.\n </li>\n<li>\n  The baseline image is transformed into a corrected color image using the pixel correspondence map.\n </li>\n<li>\n  Visual assets are linked to specific regions in the corrected color image.\n </li>\n<li>\n  Augmented reality frames are generated, aligning the visual assets with the designated regions.\n </li>\n<li>\n  Finally, these augmented reality frames are projected onto surfaces in the scene that correspond to the linked regions.\n </li>\n</ol>\n<p>\n This process enables seamless integration of digital content into physical spaces, enhancing the user experience in augmented reality applications.\n</p>\n"}, "Page 2": {"response": "<h3>\n Patent Application Overview: US 2020/0402247 A1\n</h3>\n<p>\n<strong>\n  Publication Information:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Publication Date:\n  </strong>\n  December 24, 2020\n </li>\n<li>\n<strong>\n   Patent Number:\n  </strong>\n  US 2020/0402247 A1\n </li>\n</ul>\n<p>\n<strong>\n  Description of the System:\n </strong>\n The patent describes a system for augmenting real scenes with projected visual content. The key components involved are:\n</p>\n<ol>\n<li>\n<strong>\n   Scene:\n  </strong>\n  The physical environment where augmentation takes place.\n </li>\n<li>\n<strong>\n   Projector:\n  </strong>\n  This device displays visual content onto surfaces within the scene.\n </li>\n<li>\n<strong>\n   Camera:\n  </strong>\n  Positioned to capture images of the scene, assisting in the alignment of projected content.\n </li>\n<li>\n<strong>\n   Peripheral Control Module:\n  </strong>\n  This module coordinates the projector and camera, handling the setup and processing of images.\n </li>\n</ol>\n<p>\n<strong>\n  Process Steps:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Setup Frames:\n  </strong>\n  Horizontal setup frames are projected to prepare the scene for augmentation.\n </li>\n<li>\n<strong>\n   Scan Images:\n  </strong>\n  The system captures images of the scene during projection, creating a baseline image for further processing.\n </li>\n<li>\n<strong>\n   Pixel Correspondence Map:\n  </strong>\n  A map is generated that matches pixels in the camera's view to their corresponding locations in the projected images.\n </li>\n</ul>\n<p>\n<strong>\n  Visual Asset Integration:\n </strong>\n</p>\n<ul>\n<li>\n  The system utilizes an asset library containing visual assets (like images or videos) that can be projected.\n </li>\n<li>\n  These assets are mapped to specific areas of the scene, allowing them to appear correctly aligned when projected.\n </li>\n</ul>\n<p>\n<strong>\n  Figure Overview:\n </strong>\n</p>\n<ul>\n<li>\n  The included figure illustrates the relationship between the projector's and camera's fields of view, the setup process, and how visual assets are integrated into the scene.\n </li>\n</ul>\n<p>\n This technology aims to enhance user experiences in various applications, including marketing and interactive displays, by seamlessly merging digital content with physical environments.\n</p>\n"}, "Page 3": {"response": "<h3>\n Patent Application Continuation: US 2020/0402247 A1\n</h3>\n<p>\n<strong>\n  Publication Information:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Publication Date:\n  </strong>\n  December 24, 2020\n </li>\n<li>\n<strong>\n   Patent Number:\n  </strong>\n  US 2020/0402247 A1\n </li>\n</ul>\n<p>\n<strong>\n  Scene Augmentation System:\n </strong>\n The patent outlines a system that enhances real-world scenes with projected visual content, focusing on a structured approach to ensure accuracy and effectiveness.\n</p>\n<p>\n<strong>\n  Key Components:\n </strong>\n</p>\n<ol>\n<li>\n<strong>\n   Scene:\n  </strong>\n  The physical environment being augmented.\n </li>\n<li>\n<strong>\n   Projector:\n  </strong>\n  Projects visual content onto surfaces in the scene.\n </li>\n<li>\n<strong>\n   Camera:\n  </strong>\n  Captures images of the scene to facilitate accurate alignment of projections.\n </li>\n<li>\n<strong>\n   Peripheral Control Module:\n  </strong>\n  Manages the interaction between the projector and camera.\n </li>\n</ol>\n<p>\n<strong>\n  Process Overview:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Horizontal Verification Frames:\n  </strong>\n  These frames are used to verify the accuracy of the scanned images, ensuring that projections align correctly.\n </li>\n<li>\n<strong>\n   Scan Images:\n  </strong>\n  Captured images are used to create a verification image that assists in mapping visual content to the scene.\n </li>\n</ul>\n<p>\n<strong>\n  Pixel Correspondence Map:\n </strong>\n</p>\n<ul>\n<li>\n  A critical component that establishes the relationship between the captured images from the camera and the projected content from the projector, enhancing the alignment of visual assets.\n </li>\n</ul>\n<p>\n<strong>\n  Visual Asset Integration:\n </strong>\n</p>\n<ul>\n<li>\n  The system includes background frames and an asset map that organizes visual content, such as promotions or advertisements (e.g., \"FALL SALE!\"), ensuring they are effectively displayed in the scene.\n </li>\n</ul>\n<p>\n<strong>\n  Figures:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Figure 1:\n  </strong>\n  Illustrates the relationship between the projector and camera fields of view, along with the verification process.\n </li>\n<li>\n<strong>\n   Figure 2:\n  </strong>\n  Demonstrates how background frames and adjusted asset maps contribute to the overall system functionality.\n </li>\n</ul>\n<p>\n This technology aims to create immersive experiences by accurately blending digital elements with the physical world, making it valuable for various applications such as advertising and interactive displays.\n</p>\n"}, "Page 4": {"response": "<h3>\n Patent Application Details: US 2020/0402247 A1\n</h3>\n<p>\n<strong>\n  Publication Overview:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Publication Date:\n  </strong>\n  December 24, 2020\n </li>\n<li>\n<strong>\n   Patent Number:\n  </strong>\n  US 2020/0402247 A1\n </li>\n</ul>\n<p>\n<strong>\n  Key Components Illustrated:\n </strong>\n</p>\n<ol>\n<li>\n<strong>\n   Baseline Image (S100):\n  </strong>\n  The original image of the scene used as a reference for further processing.\n </li>\n<li>\n<strong>\n   Horizontal Setup Frame (S120):\n  </strong>\n  A frame designed to assist in setting up the projection parameters for the scene.\n </li>\n<li>\n<strong>\n   Horizontal Scan Image (S102):\n  </strong>\n  An image captured during the scanning process, which aids in alignment and calibration.\n </li>\n<li>\n<strong>\n   Composite Horizontal Scan Image (S130):\n  </strong>\n  A merged image that combines multiple scan images for a comprehensive view of the scene.\n </li>\n<li>\n<strong>\n   Pixel Correspondence Map:\n  </strong>\n  This map helps establish the relationship between different images, ensuring that projected content aligns correctly with the scene.\n </li>\n</ol>\n<p>\n<strong>\n  Process Summary:\n </strong>\n</p>\n<ul>\n<li>\n  The system utilizes these components to ensure accurate augmentation of the real-world environment with digital content. By establishing a correspondence between the baseline image, setup frames, and scanned images, the system effectively aligns visual assets, enhancing user experience in augmented reality applications.\n </li>\n</ul>\n"}, "Page 5": {"response": "<h3>\n Patent Application Details: US 2020/0402247 A1\n</h3>\n<p>\n<strong>\n  Publication Overview:\n </strong>\n</p>\n<ul>\n<li>\n<strong>\n   Publication Date:\n  </strong>\n  December 24, 2020\n </li>\n<li>\n<strong>\n   Patent Number:\n  </strong>\n  US 2020/0402247 A1\n </li>\n</ul>\n<p>\n<strong>\n  Key Components Illustrated:\n </strong>\n</p>\n<ol>\n<li>\n<strong>\n   First Boundary Scan Image (S100):\n  </strong>\n  The initial image captured to define the boundaries of the scene, aiding in the identification of relevant areas for augmentation.\n </li>\n<li>\n<strong>\n   Secondary Boundary Scan Image (S120):\n  </strong>\n  A follow-up image that provides additional boundary information, enhancing accuracy in the projection setup.\n </li>\n<li>\n<strong>\n   Projector Field of View (S130):\n  </strong>\n  The area visible to the projector, indicating where augmented content will be displayed.\n </li>\n<li>\n<strong>\n   Camera Field of View:\n  </strong>\n  The range visible to the camera, crucial for capturing the scene and aligning the projections accurately.\n </li>\n<li>\n<strong>\n   Composite Boundary Scan Image:\n  </strong>\n  This image merges data from the first and secondary scans to create a comprehensive boundary representation for better projection alignment.\n </li>\n</ol>\n<p>\n<strong>\n  Process Summary:\n </strong>\n</p>\n<ul>\n<li>\n  The combination of these boundary scan images allows for precise mapping of the scene. By utilizing both the first and secondary boundary scans, the system ensures accurate detection of the areas where visual content will be projected, facilitating an effective augmented reality experience.\n </li>\n</ul>\n"}, "Page 6": {"response": "<h3>\n Patent Application Details: US 2020/0402247 A1 (Sheet 5)\n</h3>\n<p>\n<strong>\n  Key Components Illustrated:\n </strong>\n</p>\n<ol>\n<li>\n<strong>\n   Pixel Correspondence Map (S100):\n  </strong>\n  This map establishes the relationship between the camera's view and the projector's output, facilitating accurate alignment of augmented visuals with the real-world scene.\n </li>\n<li>\n<strong>\n   Data Interpolation (S130):\n  </strong>\n  This process uses the pixel correspondence map to create a mapping from the projector back to the camera view, ensuring that the projected content aligns correctly with the actual scene captured by the camera.\n </li>\n<li>\n<strong>\n   Holes in the Correspondence Map:\n  </strong>\n  This section indicates areas where the pixel correspondence may be incomplete or inaccurate, referred to as \"holes.\" Identifying these holes is crucial for improving the accuracy of the projection and ensuring seamless integration of augmented content.\n </li>\n</ol>\n<p>\n<strong>\n  Process Summary:\n </strong>\n</p>\n<ul>\n<li>\n  The pixel correspondence maps are essential for coordinating how the projector's output aligns with the camera's perspective. By interpolating data, the system compensates for any discrepancies and addresses gaps in the mapping, which enhances the overall effectiveness of the augmented reality experience.\n </li>\n</ul>\n"}, "Page 7": {"response": "<h3>\n Patent Application Details: US 2020/0402247 A1 (Sheet 6)\n</h3>\n<p>\n<strong>\n  Key Images and Descriptions:\n </strong>\n</p>\n<ol>\n<li>\n<p>\n<strong>\n    Baseline Image (S100):\n   </strong>\n   This represents the original unaltered view of the scene captured by the camera. It serves as the reference point for further processing.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Corrected Color Image (Unfilled) (S122):\n   </strong>\n   This version of the image has been adjusted for color accuracy but still lacks certain details or enhancements. It is an intermediate step in the correction process.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Corrected Color Image (Filled) (S140):\n   </strong>\n   This image builds on the previous step by not only correcting colors but also filling in any missing details or enhancements, resulting in a complete and visually rich representation of the scene.\n  </p>\n</li>\n</ol>\n<p>\n<strong>\n  Process Summary:\n </strong>\n</p>\n<ul>\n<li>\n  The progression from the baseline image to the filled corrected color image illustrates the steps taken to enhance and accurately represent the scene. First, the original image is corrected for color, and then additional details are filled in to create a polished final product, suitable for projection and augmented reality applications.\n </li>\n</ul>\n"}, "Page 8": {"response": "<h3>\n Patent Application Details: US 2020/0402247 A1 (Sheet 7)\n</h3>\n<p>\n<strong>\n  Visual Asset Management:\n </strong>\n</p>\n<ol>\n<li>\n<p>\n<strong>\n    Content Application (S100):\n   </strong>\n   This section is dedicated to managing and displaying content applications, likely related to augmented reality or visual enhancements.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Visual Asset Library (S150):\n   </strong>\n   This component houses various visual assets that can be utilized in the application. It serves as a repository for images, videos, and other graphical elements.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Creation Window (S160):\n   </strong>\n   This area provides a workspace where users can create or modify visual content. It likely includes tools for editing and arranging visual assets.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Additional Elements (S170 &amp; S180):\n   </strong>\n   These may include user interface components, settings, or options for managing the visual assets and their application within the scene.\n  </p>\n</li>\n</ol>\n<p>\n<strong>\n  Process Overview:\n </strong>\n</p>\n<ul>\n<li>\n  The diagram illustrates how different elements work together within a visual content application, allowing users to access and manage visual assets effectively. The creation window facilitates the design and modification of content, enhancing the overall user experience in augmented reality scenarios.\n </li>\n</ul>\n"}, "Page 9": {"response": "<h3>\n Patent Application Details: US 2020/0402247 A1 (Sheet 8)\n</h3>\n<p>\n<strong>\n  Scene and Visual Assets:\n </strong>\n</p>\n<ol>\n<li>\n<p>\n<strong>\n    Scene (S100):\n   </strong>\n   Represents the environment where visual content will be displayed.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Visual Assets (S180):\n   </strong>\n</p>\n<ul>\n<li>\n<strong>\n     Text Visual Asset (\"STAY WARM\"):\n    </strong>\n    A text element promoting warmth, likely for seasonal marketing.\n   </li>\n<li>\n<strong>\n     Flame Visual Asset:\n    </strong>\n    Accompanies the \"STAY WARM\" text, reinforcing the theme of warmth.\n   </li>\n<li>\n<strong>\n     Text Visual Asset (\"WINTER SALE 20% OFF\"):\n    </strong>\n    Highlights a promotional sale, indicating a discount for winter-themed products.\n   </li>\n<li>\n<strong>\n     Raindrop Visual Asset:\n    </strong>\n    May serve as a decorative or thematic element, adding to the winter ambiance.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Peripheral Control Module (S150):\n   </strong>\n   This component likely manages the interaction between the visual assets and the projection system, ensuring that the displayed content aligns with the intended design.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Content Application and Creator Window (S180):\n   </strong>\n   Provides a space for users to create, modify, and manage content within the application, facilitating the integration of visual assets into the scene.\n  </p>\n</li>\n</ol>\n<p>\n<strong>\n  Summary:\n </strong>\n The diagram illustrates a scene that incorporates various visual assets, including promotional text and thematic graphics, for a winter sale campaign. The setup emphasizes user interaction through a control module, enhancing the overall visual presentation and marketing effectiveness.\n</p>\n"}, "Page 10": {"response": "<h3>\n Patent US 2020/0402247 A1: Method for Augmenting Real-World Scenes\n</h3>\n<p>\n<strong>\n  Cross-Reference to Related Applications:\n </strong>\n This patent application is a continuation of U.S. patent applications Ser. No. 16/447,792 (filed June 20, 2019) and Ser. No. 16/146,679 (filed September 28, 2018). Both prior applications are fully incorporated by reference.\n</p>\n<p>\n<strong>\n  Technical Field:\n </strong>\n This invention pertains to augmented reality, specifically focusing on methods for enhancing real-world scenes with projected visual content.\n</p>\n<p>\n<strong>\n  Overview of the Method:\n </strong>\n The method involves several key steps:\n</p>\n<ol>\n<li>\n<strong>\n   Setup Frame Projection:\n  </strong>\n  The process begins by sending a series of setup frames to an external projector that faces the intended scene.\n </li>\n<li>\n<strong>\n   Image Capture:\n  </strong>\n  A peripheral control module equipped with a camera captures images of the scene during the projection of these frames, including a baseline image that represents the scene.\n </li>\n<li>\n<strong>\n   Pixel Correspondence Mapping:\n  </strong>\n  Using the captured images and setup frames, a pixel correspondence map is generated.\n </li>\n<li>\n<strong>\n   Image Correction:\n  </strong>\n  The baseline image is transformed into a corrected color image using the pixel correspondence map, aligning it with the perspective of the external projector.\n </li>\n<li>\n<strong>\n   Asset Linking:\n  </strong>\n  Visual assets are linked to distinct areas in the corrected color image, each area corresponding to a surface in the scene.\n </li>\n<li>\n<strong>\n   Augmented Reality Frame Generation:\n  </strong>\n  A sequence of augmented reality frames is created, showing the visual assets positioned within the corrected color image.\n </li>\n<li>\n<strong>\n   Projection of Augmented Content:\n  </strong>\n  Finally, the augmented reality frames are sent to the projector, which casts the visual content onto the real surfaces in the scene.\n </li>\n</ol>\n<p>\n<strong>\n  Variations of the Method:\n </strong>\n The patent details various flowchart representations of the method, illustrating different variations in execution and implementation.\n</p>\n<p>\n<strong>\n  Embodiments of the Invention:\n </strong>\n The description of the invention includes potential variations and configurations, emphasizing flexibility in application. The core method can be executed by a peripheral control module and a computing device to record a baseline image, transform it for projection, and manage visual assets to create a cohesive augmented reality experience.\n</p>\n<p>\n<strong>\n  Conclusion:\n </strong>\n This patent outlines a sophisticated method for merging digital content with physical environments, enabling enhanced user interactions in augmented reality applications. The approach is designed to accommodate varying projector characteristics and spatial relationships, making it adaptable for various real-world scenarios.\n</p>\n"}, "Page 11": {"response": "<h1>\n Overview of Augmented Reality Projection System (US 2020/0402247 A1)\n</h1>\n<p>\n This document outlines a system designed to create and project augmented reality (AR) content onto real-world scenes quickly and accurately. The system uses a projector, a peripheral control module, and a content application, all working together to streamline the process.\n</p>\n<h3>\n Key Components and Functionality:\n</h3>\n<ol>\n<li>\n<p>\n<strong>\n    Quick and Easy Setup:\n   </strong>\n</p>\n<ul>\n<li>\n    Users can set up AR content in a short time (usually just one scan cycle) with minimal effort. There\u2019s no need to manually identify the projector or adjust its properties, making it accessible for users who may be remote from the equipment.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    High Spatial Accuracy:\n   </strong>\n</p>\n<ul>\n<li>\n    The system ensures that animated visuals align precisely with real objects and surfaces in the scene. This high level of accuracy allows for realistic and immersive AR experiences.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Creating Color-Corrected Images:\n   </strong>\n</p>\n<ul>\n<li>\n    The peripheral control module uses a 2D camera to capture scan images and constructs a high-resolution, accurate color representation of the projector\u2019s field of view. This \"corrected color image\" helps users identify where to place digital assets within the scene.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    User Interaction and Asset Placement:\n   </strong>\n</p>\n<ul>\n<li>\n    Users can choose and assign visual assets from a library to specific surfaces in the corrected color image. The system compiles these selections into AR frames, which the projector can then display, accurately aligning with real-world objects.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Automatic Adjustments:\n   </strong>\n</p>\n<ul>\n<li>\n    The system can automatically determine the position of the camera relative to the projector\u2019s field of view. It creates a 3D representation of the scene, enabling the system to dynamically adjust and accurately project visuals onto the corresponding surfaces.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Minimal Hardware Requirements:\n   </strong>\n</p>\n<ul>\n<li>\n    The system is designed to operate without the need for expensive equipment, such as multiple cameras or complex setup procedures. This makes it more cost-effective and user-friendly.\n   </li>\n</ul>\n</li>\n</ol>\n<h3>\n Peripheral Control Module Features:\n</h3>\n<ul>\n<li>\n<strong>\n   Camera:\n  </strong>\n  A 2D color camera with a wide field of view that can capture more of the scene than the projector can display, making setup easier.\n </li>\n<li>\n<strong>\n   Processor:\n  </strong>\n  Responsible for executing various tasks and managing the AR content.\n </li>\n<li>\n<strong>\n   Data and Video Ports:\n  </strong>\n  Facilitate communication between the peripheral control module, projector, and computing device.\n </li>\n<li>\n<strong>\n   Housing:\n  </strong>\n  Designed to be easily mounted on the projector, making the system compact and efficient.\n </li>\n</ul>\n<h3>\n Conclusion:\n</h3>\n<p>\n The described AR projection system simplifies the process of creating and displaying augmented reality content. By integrating user-friendl\n</p>\n"}, "Page 12": {"response": "<h3>\n US 2020/0402247 A1: Peripheral Control Module and Setup Frame Methodology\n</h3>\n<p>\n<strong>\n  Mounting Configurations:\n </strong>\n The peripheral control module can be attached to various parts of a projector\u2014such as the bottom, side, or front\u2014ensuring that its camera faces a scene also visible to the projector. Common mounting methods include using double-sided tape or hook-and-loop strips, or through mechanical fastening to adjacent structures like wall or ceiling mounts.\n</p>\n<p>\n<strong>\n  Housing Design and Camera Orientation:\n </strong>\n In a specific implementation, the module features an elongated rectilinear housing with the camera positioned at one corner. This design allows for multiple mounting orientations:\n</p>\n<ul>\n<li>\n<p>\n<strong>\n    First Horizontal Orientation:\n   </strong>\n   The camera's corner is positioned closest to the projector, minimizing the offset distance between their fields of view. This configuration reduces the overlap of visible areas, ensuring both camera and projector capture the same scene components.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Second Horizontal Orientation:\n   </strong>\n   The camera corner is placed opposite the projector body, which increases the offset. While this enhances the accuracy of the disparity map calculation, it does increase the potential for mismatched visibility between the camera and projector fields.\n  </p>\n</li>\n<li>\n<p>\n<strong>\n    Third Vertical Orientation:\n   </strong>\n   Here, the camera is maximally distanced from the projector, allowing for the most accurate disparity map calculations, albeit with further increased visibility discrepancies.\n  </p>\n</li>\n</ul>\n<p>\n The module is versatile enough to be set on any nearby surface within the projector's field of view.\n</p>\n<p>\n<strong>\n  Content Application and Virtual Environment:\n </strong>\n The content application is designed to run on various computing devices (desktops, laptops, tablets, smartphones) and creates a virtual workspace where users can link visual assets\u2014both custom and from a library\u2014to specific areas in a corrected color image. This application supports features like:\n</p>\n<ul>\n<li>\n  Hosting a creator window to visualize corrected images.\n </li>\n<li>\n  Providing tools for selecting, adjusting, and associating visual assets with discrete scene elements.\n </li>\n</ul>\n<p>\n<strong>\n  Setup Frame Generation:\n </strong>\n The peripheral control module generates setup frames, crucial for establishing the projector's operational parameters. For instance, a horizontal setup frame is created that encodes pixel values in binary format, with \u201c0\u201d and \u201c1\u201d designating black and white pixels, respectively. This process is repeated for multiple frames to cover various pixel positions, ensuring comprehensive setup for each pixel column and row.\n</p>\n<ul>\n<li>\n  For a projector with a resolution of 1280x800, the peripheral control module initializes a corresponding array of setup images. Each pixel in these arrays is encoded with unique binary values that correspond to specific visual patterns, facilitating effective calibration.\n </li>\n</ul>\n<p>\n<strong>\n  Conclusion:\n </strong>\n The innovative design and operational methodology of the peripheral control module enable enhanced accuracy in projecting augmented reality content. As technology evolves, these systems will likely integrate even more advanced functionalities, further refining the interplay between real-world visuals and digital augmentations. This adaptability is crucial for maintaining relevance in an ever-evolving technological landscape.\n</p>\n"}, "Page 13": {"response": "<h3>\n US 2020/0402247 A1: Horizontal and Vertical Setup Frames Generation\n</h3>\n<p>\n<strong>\n  Horizontal Setup Frames:\n </strong>\n In a projector with a horizontal resolution of 1280 pixels, the peripheral control module generates multiple horizontal setup frames, each defined by specific binary patterns. For example:\n</p>\n<ul>\n<li>\n<strong>\n   Eleventh Horizontal Setup Frame:\n  </strong>\n  All pixel columns with a leftmost bit value of \u201c0\u201d (e.g., \u201c0xxxxXXXXXX\u201d) are set to black, while those with a leftmost bit value of \u201c1\u201d (e.g., \u201c1xxxxxxxxxx\u201d) are set to white.\n </li>\n<li>\n<strong>\n   Tenth Horizontal Setup Frame:\n  </strong>\n  In this frame, all pixel columns with a second leftmost bit value of \u201c0\u201d (e.g., \u201cx0xxxxxxxxx\u201d) are black, and those with a second leftmost bit of \u201c1\u201d (e.g., \u201cx1xxxxxxxxx\u201d) are white.\n </li>\n<li>\n<strong>\n   Ninth Horizontal Setup Frame:\n  </strong>\n  Similar logic applies, where pixel columns with a third leftmost bit of \u201c0\u201d are black, and those with \u201c1\u201d are white.\n </li>\n</ul>\n<p>\n This pattern continues down to the first horizontal setup frame, where all pixel columns with a first rightmost bit value of \u201c0\u201d (e.g., \u201cxxxxxxxxx0\u201d) are black and those with \u201c1\u201d are white.\n</p>\n<p>\n<strong>\n  Example Configurations:\n </strong>\n</p>\n<ul>\n<li>\n  The eleventh frame might have its left half in black and the right half in white.\n </li>\n<li>\n  The tenth frame could have alternating vertical quartiles, with the first and third in black and the second and fourth in white.\n </li>\n<li>\n  The ninth frame may alternate black and white across vertical octiles.\n </li>\n</ul>\n<p>\n<strong>\n  Vertical Setup Frames:\n </strong>\n The peripheral control module also generates vertical setup frames for a projector with a vertical resolution of 800 pixels. Each frame is defined by:\n</p>\n<ul>\n<li>\n<strong>\n   Tenth Vertical Setup Frame:\n  </strong>\n  Here, pixel rows with a first topmost bit value of \u201c0\u201d (e.g., \u201c0xxxxxxxxx\u201d) are black, and those with \u201c1\u201d are white.\n </li>\n<li>\n<strong>\n   Ninth Vertical Setup Frame:\n  </strong>\n  This frame features rows with a second topmost bit of \u201c0\u201d as black and \u201c1\u201d as white.\n </li>\n</ul>\n<p>\n As with the horizontal frames, this pattern allows for complex configurations, including:\n</p>\n<ul>\n<li>\n  The tenth vertical frame with the bottom half in black and the top half in white.\n </li>\n<li>\n  A ninth frame where the first and third vertical quartiles are black and the second and fourth are white.\n </li>\n</ul>\n<p>\n<strong>\n  Setup Frame Generation:\n </strong>\n The peripheral control module can generate these setup frames in various configurations and graycode patterns, tailoring the output to the specific needs of the projector.\n</p>\n<p>\n<strong>\n  Scan Cycle Methodology:\n </strong>\n The system follows a defined sequence for using these setup frames:\n</p>\n<ol>\n<li>\n<strong>\n   Output Setup Frames:\n  </strong>\n  Each setup frame is sent to the projector in succession.\n </li>\n<li>\n<strong>\n   Record Scan Images:\n  </strong>\n  After projecting a setup frame, the camera captures a corresponding scan image.\n </li>\n<li>\n<strong>\n   Store Images:\n  </strong>\n  Each scan image is saved for further processing, along with a baseline image depicting the scene.\n </li>\n</ol>\n<p>\n This process includes executing a horizontal scan cycle where horizontal setup frames are output one after another, triggering the camera to record a 2D color image for each frame. The images are stored locally or processed further by the content application.\n</p>\n<p>\n This systematic approach ensures accurate calibration and mapping of the projector's output in relation to the observed scene.\n</p>\n"}, "Page 14": {"response": "<h3>\n US 2020/0402247 A1: Vertical Scan Cycle and Boundary Mapping\n</h3>\n<p>\n<strong>\n  Vertical Scan Cycle:\n </strong>\n After completing the horizontal scan cycle, the peripheral control module initiates a vertical scan cycle. This involves:\n</p>\n<ol>\n<li>\n<strong>\n   Outputting Vertical Setup Frames:\n  </strong>\n  Each vertical setup frame is sequentially sent to the projector.\n </li>\n<li>\n<strong>\n   Recording Scan Images:\n  </strong>\n  The camera captures a 2D color scan image for each vertical setup frame.\n </li>\n<li>\n<strong>\n   Storing Scan Images:\n  </strong>\n  These images, referred to as the \"set of vertical scan images,\" are saved in local memory or sent to the content application for further processing.\n </li>\n</ol>\n<p>\n The control module can also alternate between horizontal and vertical setup frames during this process, allowing for flexibility in how the images are recorded.\n</p>\n<p>\n<strong>\n  Baseline Image Recording:\n </strong>\n In a separate step, the control module records a baseline image of the scene. This can occur before, during, or after the vertical and horizontal scans. For instance, the camera may capture this image when the projector is off or projecting an all-black frame. Alternatively, a white frame can be cast to capture the baseline image during projection.\n</p>\n<p>\n<strong>\n  Boundary Mapping Frames:\n </strong>\n The control module generates boundary mapping frames to limit light projection and glare while capturing images of the scene. Here's how it works:\n</p>\n<ul>\n<li>\n<strong>\n   First Boundary Mapping Frame:\n  </strong>\n  This frame features a sparse grid of bright white dots on a black background, projecting minimal light.\n </li>\n<li>\n<strong>\n   Second Boundary Mapping Frame:\n  </strong>\n  This is the inverse, designed to produce greater reflected light and more glare.\n </li>\n</ul>\n<p>\n When capturing these frames, the control module takes care to minimize reflections that could confuse pixel correspondences between the projector and camera.\n</p>\n<p>\n<strong>\n  Processing Boundary Scan Images:\n </strong>\n After capturing boundary scan images corresponding to the two mapping frames, the control module processes these images by converting them to black and white and performing pixel-wise subtraction. This generates a composite boundary scan image, where:\n</p>\n<ul>\n<li>\n  High pixel values correspond to areas illuminated by the first boundary mapping frame.\n </li>\n<li>\n  Low pixel values indicate regions not illuminated by either frame.\n </li>\n</ul>\n<p>\n<strong>\n  Determining the Projector's Field of View:\n </strong>\n The peripheral control module uses these composite images to derive the boundaries of the projector's field of view, especially in scenes with reflective surfaces. By capturing a series of boundary mapping frames and their corresponding images, the system identifies the area where light reflections occur.\n</p>\n<p>\n<strong>\n  Calculating the Perimeter:\n </strong>\n The module isolates clusters of pixels in the composite image that exceed a certain brightness threshold and calculates a perimeter encompassing these clusters. This perimeter is stored as the boundary of the projector's field of view.\n</p>\n<p>\n<strong>\n  Cropping and Transforming Images:\n </strong>\n Once the boundary is established, the peripheral control module can crop scan images or correspondence maps to align with this perimeter. It transforms pixels within the boundary in the baseline image to create a corrected color image, ensuring that no irrelevant pixel correspondences are drawn from outside the projector's field of view. This meticulous process enhances the accuracy of the projected images in relation to the actual scene.\n</p>\n"}, "Page 15": {"response": "<h3>\n US 2020/0402247 A1: Pixel Correspondence Mapping Between Camera and Projector\n</h3>\n<p>\n<strong>\n  Calculating Pixel Correspondence:\n </strong>\n In Block S130 of the method, the peripheral control module generates a pixel correspondence map. This map connects pixels in the camera's field of view to corresponding pixels in the projector's field, based on values in scan images and their associated setup frames. The module can either calculate this map directly or provide the necessary data to an application that can derive it.\n</p>\n<p>\n When the projector outputs a horizontal setup frame, it specifies pixel values. For instance, the first horizontal setup frame might assign a value of \u201c1\u201d (white) to all pixels that meet a certain binary criterion. The control module then processes the corresponding black-and-white horizontal scan image, weighting each pixel according to the order of the setup frame.\n</p>\n<p>\n<strong>\n  Generating Composite Horizontal Scan Images:\n </strong>\n The control module sums the weighted black-and-white images to create a composite horizontal scan image. Each pixel in this composite corresponds to a specific column in the projector. For example, in a projector with a horizontal resolution of 1280 pixels, a pixel value of \u201c0\u201d in the composite image correlates with the first projector column, while a value of \u201c1279\u201d corresponds to the last column.\n</p>\n<p>\n<strong>\n  Horizontal Pixel Correspondence Example:\n </strong>\n The peripheral control module converts each horizontal scan image into a black-and-white format. These images are then compiled into a horizontal composite scan image, where each pixel is linked to a specific address in both the camera and the projector. The module also weights these images based on the corresponding setup frame\u2019s order.\n</p>\n<p>\n For example, while recording an eleventh horizontal scan image, the corresponding setup frame can designate a pixel value of \u201c1\u201d for certain columns. The module then weighs this scan image, multiplying each pixel by\n 2\n (\n 11\n \u2212\n 1\n )\n 2^{(11-1)}\n 2\n (\n 11\n \u2212\n 1\n )\n or 1024, in accordance with the frame order.\n</p>\n<p>\n<strong>\n  Vertical Pixel Correspondence:\n </strong>\n The process for vertical pixel correspondence is similar. In Block S130, the control module converts vertical scan images into black-and-white images and compiles these into a vertical composite scan image. Each pixel in this composite is assigned an address corresponding to the camera's pixels and contains a vertical pixel value tied to a row of pixels in the projector.\n</p>\n<p>\n For instance, when the tenth vertical setup frame is active, it might set a pixel value of \u201c1\u201d for specific rows. The control module then weighs this vertical scan image by multiplying each pixel by\n 2\n (\n 10\n \u2212\n 1\n )\n 2^{(10-1)}\n 2\n (\n 10\n \u2212\n 1\n )\n or 512, based on the setup frame's order.\n</p>\n<p>\n<strong>\n  Example of Vertical Scan Image Processing:\n </strong>\n As in the horizontal case, the peripheral control module records and processes vertical scan images in a structured manner. For the ninth vertical setup frame, it assigns pixel values similarly, ensuring the mapping and weighting reflect the proper relationship between the camera and projector's fields of view. This methodical approach allows for accurate correspondence between the two domains.\n</p>\n"}, "Page 16": {"response": "<h3>\n US 2020/0402247 A1: Vertical Pixel Correspondence and Composite Image Generation\n</h3>\n<p>\n<strong>\n  Vertical Pixel Correspondence Processing:\n </strong>\n The peripheral control module continues the process of generating vertical pixel correspondence by recording a vertical scan image when the projector outputs the first vertical setup frame. This frame designates specific pixel values, typically \u201c1\u201d (white), for pixel rows that meet certain binary criteria. The module then converts this scan image into black-and-white format, applying a weighting factor based on the setup frame's order.\n</p>\n<p>\n The module sums all the weighted black-and-white vertical scan images to create a composite vertical scan image. Each pixel in this composite directly corresponds to a pixel row in the projector. For example, in a projector with a vertical resolution of 800 pixels, a pixel value of \u201c0\u201d in the composite image corresponds to the first row of the projector, while a value of \u201c799\u201d correlates with the last row.\n</p>\n<p>\n<strong>\n  Detectable Tile Limits:\n </strong>\n As the number of vertical tiles increases in horizontal setup frames, the width of each tile in the corresponding horizontal scan image decreases. At a certain threshold, a horizontal scan image may depict the scene at a reduced intensity (50%) instead of distinct alternating black and white tiles. To manage this, the peripheral control module can set a grayscale threshold significantly higher than the median value\u2014such as \u201c180\u201d for a 256-bit grayscale channel\u2014when converting to black and white.\n</p>\n<p>\n Alternatively, the module can use computer vision techniques to analyze each horizontal scan image for clearly defined black and white tiles. If it determines that further scans do not yield distinguishable tiles due to camera resolution limits, it discards the most recent horizontal scan image and processes the previous ones to create the composite image.\n</p>\n<p>\n<strong>\n  Resolution Compensation Techniques:\n </strong>\n When generating the composite horizontal scan image without data from higher-order setups, clusters of adjacent pixels with similar values may appear. To address this reduced resolution, the peripheral control module can compress these clusters by keeping a representative pixel (or an average of similar values) at the center of the cluster and marking the others with a \"null\" value. This allows for more accurate pixel correspondence between the camera and projector domains.\n</p>\n<p>\n The module can also apply similar methods to vertical scan images during the processing cycle, ensuring that correspondence calculations account for sparse pixel sets in both the camera and projector.\n</p>\n<p>\n<strong>\n  Creating the Pixel Correspondence Map:\n </strong>\n The peripheral control module combines the black-and-white composite horizontal and vertical scan images to form a pixel correspondence map. This map links pixels from the camera domain to specific pixels in the projector domain. Once established, the module applies this map to a color scan image captured by the camera, creating a corrected color image that reflects the scene as seen through the projector's field of view.\n</p>\n<p>\n In one implementation, the module initializes an empty pixel correspondence map, where each pixel corresponds to an (x, y) address in the camera domain and contains a null (x', y') value for the projector domain. As it processes the black-and-white composite images, the module fills in this correspondence map, establishing connections between the two domains.\n</p>\n"}, "Page 17": {"response": "<h3>\n US 2020/0402247 A1: Filling Holes in Pixel Correspondence Maps\n</h3>\n<p>\n<strong>\n  Transferring Pixel Values:\n </strong>\n The peripheral control module transfers pixel values from the black-and-white composite horizontal scan image to the x'-component of the corresponding pixels in the pixel correspondence map. Similarly, it writes pixel values from the vertical composite scan to the y'-component of the corresponding pixels. For locations outside the projector's derived field of view, the module can overwrite these entries with null values.\n</p>\n<p>\n<strong>\n  Pixel Correspondence Verification:\n </strong>\n To ensure accuracy, the peripheral control module can check the pixel correspondence map for feasible correspondences between camera and projector pixels in 3D space, possibly by calculating a fundamental matrix. Any correspondences that don't meet physical consistency criteria are removed, creating \"holes\" in the map.\n</p>\n<p>\n<strong>\n  Interpolating Across Holes:\n </strong>\n Instead of relying on the pixel correspondence map to convert a color scan image directly, the peripheral control module can interpolate spatial correspondences across these holes. For example, if a contiguous group of horizontally adjacent pixels contains null x'-component values, the module can extract values from pixels immediately adjacent on either side and linearly interpolate values across the group.\n</p>\n<p>\n In another implementation, the module calculates a disparity map comparing the baseline image to an initial corrected color image. It identifies groups of null x'-component pixels and uses depth information to extrapolate values from nearby pixels. If the surrounding pixels are determined to be on different surfaces, the module can fill the hole by predicting it\u2019s due to a shadow, thereby selecting appropriate x'-values to interpolate.\n</p>\n<p>\n<strong>\n  Addressing Depth Variations:\n </strong>\n Holes may also arise when surfaces at varying depths obscure one another due to the physical offset between the camera and projector. The camera may have a lower resolution and wider angle than the projector, leading to scenarios where a single camera pixel corresponds to multiple projector pixels. This results in additional holes due to mismatched data from the camera and projector.\n</p>\n<p>\n The peripheral control module can further interpolate between verified correspondences in the pixel correspondence map to fill these holes, ensuring more accurate mapping between the camera and projector pixels.\n</p>\n"}, "Page 18": {"response": "<h3>\n US 2020/0402247 A1: Techniques for Filling Holes and Rendering Images\n</h3>\n<p>\n<strong>\n  Filling Holes in the Pixel Correspondence Map:\n </strong>\n The peripheral control module can use various techniques to fill gaps in the pixel correspondence map, specifically targeting contiguous clusters of pixels with null values. Once these holes are filled, the module can store the final, updated pixel correspondence map in local memory or return it along with the baseline image to the content application.\n</p>\n<p>\n<strong>\n  Disparity Map Calculation:\n </strong>\n To generate a disparity map for the scene, the content application or peripheral control module first creates a pixel correspondence map that links pixel locations between the camera and projector. This map is then rectified by applying perspective transformations to both the camera and projector domains. The differences in the x-components of these transformations are stored as the disparity map, which can also be rendered as an image for visualization.\n</p>\n<p>\n<strong>\n  Using the Disparity Map:\n </strong>\n The disparity map provides a proxy for the depth of surfaces in the scene, allowing the system to fill holes in the pixel correspondence map and isolate distinct surfaces depicted in the corrected color image.\n</p>\n<p>\n<strong>\n  Rendering the Corrected Color Image:\n </strong>\n The content application renders a corrected color image that reflects the projector's current field of view within a creator window on a computing device. This interface allows users to augment surfaces in the scene by linking visual assets to specific regions in the corrected color image. The application can then generate augmented reality frames, projecting these visual assets onto the scene with high spatial accuracy.\n</p>\n<p>\n<strong>\n  Feature Detection in the Corrected Color Image:\n </strong>\n To enhance the corrected color image, the content application may implement computer vision techniques to detect objects. This includes using blob detection to identify regions with distinct properties, such as color or brightness, and edge detection to outline the boundaries of surfaces or objects. The application can also analyze the disparity map to estimate depths and distinguish objects based on depth discontinuities, thereby accurately aligning detected regions in the corrected color image with nearby edges.\n</p>\n<p>\n<strong>\n  Variation with a Second Sensor:\n </strong>\n In some implementations, the peripheral control module includes a second sensor, like an additional 2D color camera or depth sensor, positioned at a known offset from the primary camera. This setup allows the system to capture pairs of scan images, which are processed to generate a 3D representation of the scene. From this 3D data, a pixel correspondence map can be created, further refining the process of filling holes and enhancing image rendering.\n</p>\n"}, "Page 19": {"response": "<h3>\n US 2020/0402247 A1: Visual Asset Management and Augmented Reality\n</h3>\n<p>\n<strong>\n  Discrete Regions and Visual Assets:\n </strong>\n The content application identifies groups of pixels, known as \"discrete regions,\" within the corrected color image. These regions represent various objects, surfaces, or color areas in the scene.\n</p>\n<p>\n<strong>\n  Library of Visual Assets:\n </strong>\n The application features a library of static and animated visual assets, such as:\n</p>\n<ul>\n<li>\n  Animated raindrops\n </li>\n<li>\n  Animated fire\n </li>\n<li>\n  A color-changing worm that follows a path\n </li>\n<li>\n  Animated waterfalls\n </li>\n</ul>\n<p>\n Users can upload custom visual assets, including:\n</p>\n<ul>\n<li>\n  Custom graphics\n </li>\n<li>\n  Text strings with animations (e.g., bouncing or shimmering)\n </li>\n<li>\n  Animated color patterns that define boundaries and objects that change over time\n </li>\n</ul>\n<p>\n Additionally, users can utilize an asset creator tool to design new visual assets or modify existing ones for the library.\n</p>\n<p>\n<strong>\n  Effects Library:\n </strong>\n The content application includes an effects library, allowing users to assign various effects to visual assets. These effects can be:\n</p>\n<ul>\n<li>\n<strong>\n   Entry Effects:\n  </strong>\n  (e.g., wipe in, drift in)\n </li>\n<li>\n<strong>\n   Exit Effects:\n  </strong>\n  (e.g., dissolve, fireworks)\n </li>\n<li>\n<strong>\n   Maintain Effects:\n  </strong>\n  (e.g., sparkle, shimmer)\n </li>\n</ul>\n<p>\n Users can define triggers or timers for these effects, such as linking an entry effect to motion detected in the scene.\n</p>\n<p>\n<strong>\n  Dynamic Visual Assets:\n </strong>\n Visual assets can respond dynamically, adjusting based on parameters like size or position. For instance, an asset may animate faster or change color over time based on its rendering duration. Additionally, it can display animated patterns that vary according to the depth of surfaces in the scene.\n</p>\n<p>\n<strong>\n  Asset Selection and Linking:\n </strong>\n Users can select objects from the corrected color image and link visual assets from the virtual library to corresponding discrete regions. The application allows users to adjust the boundaries of these regions by dragging edges or vertices.\n</p>\n<p>\n<strong>\n  Depth Estimation and Animation:\n </strong>\n The system calculates disparity maps to estimate depth values for objects within the scene. These depth values influence the animation of visual assets, allowing for color or pattern changes based on depth gradients across the objects.\n</p>\n<p>\n<strong>\n  Rendering and Communication:\n </strong>\n The content application renders the selected visual assets over discrete regions in the corrected color image within the creator window. This process helps users visualize how assets will appear when projected onto the scene, allowing for the integration of augmented content in both the background and foreground.\n</p>\n<p>\n<strong>\n  Generating Augmented Reality Frames:\n </strong>\n In Block S170, the application generates a sequence of augmented reality frames that align the visual assets with their respective discrete regions in the corrected color image. This ensures that the visual assets are projected with high spatial accuracy onto their corresponding surfaces.\n</p>\n<p>\n<strong>\n  Frame Generation Implementation:\n </strong>\n In one implementation, the application replays visual assets within the creator window, then produces a sequence of frames at the projector's resolution and frame rate (e.g., 24 or 60 frames per second). The sequence is pre-rendered on the computing device and uploaded to the peripheral control module for projection.\n</p>\n<p>\n By following these steps, the content application effectively populates the corrected color image with a variety of dynamic visu\n</p>\n"}, "Page 20": {"response": "<h3>\n US 2020/0402247 A1: Augmented Reality Frame Streaming and Content Projection\n</h3>\n<p>\n<strong>\n  Streaming Augmented Reality Frames:\n </strong>\n The peripheral control module can stream augmented reality frames to a projector, continuously looping the output while both the projector and control module are active.\n</p>\n<p>\n<strong>\n  Background and Foreground Visual Assets:\n </strong>\n In one implementation, the content application generates:\n</p>\n<ul>\n<li>\n<strong>\n   Background Frames:\n  </strong>\n  These depict a set of visual assets aligned with discrete regions representing immutable surfaces in the scene.\n </li>\n<li>\n<strong>\n   Asset Map:\n  </strong>\n  This outlines locations for a second subset of visual assets linked to mutable surfaces or associated with entry and exit triggers.\n </li>\n</ul>\n<p>\n The application outputs these background frames, the asset map, and the second subset of visual assets to the peripheral control module, which stores this augmented content locally.\n</p>\n<p>\n<strong>\n  Real-Time Frame Rendering:\n </strong>\n The peripheral control module uses the asset map to create overlay masks for the second subset of visual assets, responsive to detected triggers (e.g., motion from a sensor). These masks are then combined with the corresponding background frames to produce augmented reality frames in near real-time.\n</p>\n<p>\n<strong>\n  Asset Management Process:\n </strong>\n The content application creates an asset map detailing the corrected color image\u2019s regions and surfaces within the projector\u2019s field of view. It populates this map with effects and triggers for the visual assets, bundles the map with asset files, and uploads it to the peripheral control module for storage.\n</p>\n<p>\n When the peripheral control module is active, it can transform the asset map, effects, triggers, and visual assets into frame overlays. These overlays combine with pre-rendered background frames to generate augmented reality frames that are output to the projector.\n</p>\n<p>\n<strong>\n  Setup and Correction Process:\n </strong>\n The system outputs setup frames onto a display, records scan images, and derives a pixel correspondence map. This map aids in generating a corrected color image and detecting edges within that image. The application highlights discrete regions, such as individual bottles, allowing users to drag animated assets over these areas to create links for augmented content.\n</p>\n<p>\n<strong>\n  Confirmation and Streaming of Augmented Content:\n </strong>\n Once the user confirms the augmented content for a display (like bottles), the application generates a sequence of frames representing the animations and sends them to the peripheral control module for local storage. The module then streams these frames to the projector, enhancing the bottles with the animations while ensuring they stay within the projector's field of view.\n</p>\n<p>\n<strong>\n  Content Projection and Examples:\n </strong>\n In Block S180, the method describes outputting augmented reality frames to the projector. The peripheral control module or content application can trigger these outputs continuously or in response to detected conditions (e.g., motion).\n</p>\n<p>\n<strong>\n  Example 1: Retail Display Setup:\n </strong>\n In a retail environment, a projector is placed on a table facing a display with items like a firepit and camping chairs. The system calculates a corrected color image of the scene and identifies discrete regions for each item. The user can drag animated assets, such as raindrops, onto the display, modifying effects like raindrops bouncing and adding glowing effects to other items.\n</p>\n<p>\n<strong>\n  Example 2: Bar Display Setup:\n </strong>\n In a bar setting, a projector is mounted from the ceiling facing a liquor bottle display. The same processes of identifying discrete regions, linking visual assets, and defining animations occur to create an engaging augmented reality experience for the displayed bottles.\n</p>\n"}, "Page 21": {"response": "<h3>\n US 2020/0402247 A1: Automated Augmented Reality Updates and Scene Adjustments\n</h3>\n<p>\n<strong>\n  Motion Detection and Augmented Content Confirmation:\n </strong>\n When a user confirms the augmented content for a retail display, the content application generates background frames featuring the second animated visual asset and relevant text. It also creates an asset map for the first animated visual asset and glowing effects, publishing these to the peripheral control module for local storage. The module continuously streams these frames to the projector, which displays them to enhance the firepit with flame animations.\n</p>\n<p>\n If a motion sensor detects movement nearby (like a customer), the peripheral control module produces overlay frames that incorporate the first animated visual asset and glowing effects. These overlays are then combined with the background frames to create new augmented reality frames, which are projected for a specified duration (e.g., one minute).\n</p>\n<p>\n<strong>\n  Handling Scene Changes:\n </strong>\n The peripheral control module monitors for changes in the scene. If the change exceeds a predefined threshold, it can adapt the augmented reality frames accordingly. This process involves capturing multiple verification images to analyze the scene's dynamics, filtering out any outlier changes to ensure only significant alterations are addressed.\n</p>\n<p>\n<strong>\n  Direct Updates from Verification Images:\n </strong>\n In instances where a change is detected between baseline and verification images, the peripheral control module performs the following:\n</p>\n<ol>\n<li>\n  Crops both images based on the projector's field of view.\n </li>\n<li>\n  Calculates a warp that aligns features in the baseline image with those in the verification image, verifying the change if the warp exceeds a set threshold.\n </li>\n<li>\n  Transforms this warp from the camera domain to the projector domain using a previously generated pixel correspondence map.\n </li>\n<li>\n  Applies the warp to create adjusted augmented reality frames, which can also be applied to the asset map and background frames to ensure consistency.\n </li>\n</ol>\n<p>\n These adjusted frames are then served to the projector, allowing it to cast updated visual assets onto surfaces that have shifted since the last image capture.\n</p>\n<p>\n<strong>\n  Automatic Updates:\n </strong>\n In some scenarios, the peripheral control module can automatically detect changes in the position of objects within the scene or the position of the projector itself. This detection triggers automatic adjustments to the positions of corresponding assets in the augmented reality frames.\n</p>\n<p>\n<strong>\n  Verification Image Capture:\n </strong>\n The peripheral control module can periodically capture verification images, either on user command, at scheduled intervals, or after detecting motion followed by a period of inactivity. To minimize differences caused by ambient light, a \"white\" frame may be projected to illuminate the scene during this process.\n</p>\n<p>\n Once a verification image is captured, it is compared to the baseline image to identify any changes in the scene. Techniques for motion detection help determine if objects have moved, and to what extent, allowing for necessary adjustments in the augmented reality projections.\n</p>\n<p>\n<strong>\n  Second Scan Cycle:\n </strong>\n A second scan cycle may be executed to gather a new set of images and derive an updated pixel correspondence map. This helps calculate a new disparity map and generate a corrected color image reflecting the current scene. The peripheral control module then compares the new disparity map with the original to verify that significant surfaces are still present and have not changed position beyond an acceptable threshold.\n</p>\n<p>\n If all criteria are met, the module calculates a warp that maps the original corrected color image to the new one. Should this warp be minor, it applies the warp to realign the augmented reality frames, ensuring that the projected visual assets accurately reflect the current scene configuration.\n</p>\n"}, "Page 22": {"response": "<h3>\n US 2020/0402247 A1: Responsive Augmented Reality Adjustment Mechanisms\n</h3>\n<p>\n<strong>\n  Dynamic Adjustments to Augmented Reality Frames:\n </strong>\n The peripheral control module is capable of modifying augmented reality frames in response to changes in the position or removal of surfaces of interest within the scene. Before sending these frames to the projector, the module ensures that they correspond accurately to the relevant surfaces.\n</p>\n<p>\n If a significant surface has changed its depth, horizontal, or vertical position beyond a set threshold since the last scan, the module prompts the user to relink the visual assets to the newly adjusted regions in the corrected color image. This prompt may be communicated through the content application or as a projected frame.\n</p>\n<p>\n<strong>\n  Second Scan Cycle for Enhanced Accuracy:\n </strong>\n The peripheral control module may execute a second scan cycle where it:\n</p>\n<ol>\n<li>\n  Serves a new sequence of setup frames to the projector.\n </li>\n<li>\n  Records a second set of scan images during the projection of these setup frames.\n </li>\n<li>\n  Captures a verification image of the scene.\n </li>\n<li>\n  Calculates a new pixel correspondence map from the scan images and setup frames.\n </li>\n<li>\n  Transforms the verification image into a corrected color image based on the new pixel map.\n </li>\n</ol>\n<p>\n Using this second corrected image, the module can warp the asset map to align specific visual assets with corresponding features detected in the new scene. This involves applying computer vision techniques to identify relevant features, edges, and objects, allowing for accurate linkage to surfaces with similar attributes.\n</p>\n<p>\n<strong>\n  Overlaying Adjusted Augmented Reality Frames:\n </strong>\n The peripheral control module can generate overlay masks from the warped asset map and apply these to unchanged background frames, creating a sequence of adjusted augmented reality frames. These frames are then served to the projector to project visual assets accurately onto surfaces that have moved since the previous scan.\n</p>\n<p>\n The regions associated with visual assets may correspond to user-specified surfaces in the projector's field of view. The module compares the augmented regions with features detected in the scene to determine any positional or orientation changes. If any surfaces of interest have shifted, the module automatically adjusts the augmented reality frames accordingly.\n</p>\n<p>\n<strong>\n  Handling Missing Correspondences:\n </strong>\n If the peripheral control module does not detect a surface in the current scene that matches an augmented region from the asset map, it will remove that region. Conversely, if it cannot find a correspondence between detected features and the asset map, it will store the new corrected color image and prompt the user to relink visual assets to the updated regions.\n</p>\n<p>\n<strong>\n  System Implementation:\n </strong>\n The described systems and methods can be implemented as a machine that utilizes a computer-readable medium containing executable instructions. These instructions can run on various devices, including computers, mobile devices, or dedicated hardware.\n</p>\n<p>\n The methods for augmenting a real space scene with projected content involve several steps:\n</p>\n<ul>\n<li>\n  Projecting a sequence of setup frames onto the scene.\n </li>\n<li>\n  Capturing scan images while projecting these frames.\n </li>\n<li>\n  Calculating a pixel correspondence map from the scan images.\n </li>\n<li>\n  Creating a 3D representation of the scene.\n </li>\n<li>\n  Accessing and warping visual assets based on the 3D representation.\n </li>\n</ul>\n<p>\n These processes enable dynamic and accurate augmented reality experiences that adapt to changes in the physical environment.\n</p>\n"}, "Page 23": {"response": "<h3>\n US 2020/0402247 A1: Method for Projecting Visual Assets in Augmented Reality\n</h3>\n<p>\n<strong>\n  Projecting Visual Assets:\n </strong>\n The process begins by projecting a first visual asset onto a designated region of a scene using a light projector.\n</p>\n<p>\n<strong>\n  Detecting Depth Discontinuities:\n </strong>\n The method involves detecting depth discontinuities in a three-dimensional representation of the scene. This allows for distinguishing the first region from a second region based on these depth changes. The first visual asset is then scaled appropriately for the first region.\n</p>\n<p>\n<strong>\n  Using Color Gradients for Alignment:\n </strong>\n Additionally, the process includes capturing a baseline image of the scene and transforming it into a corrected color image that reflects the projector's perspective. The method detects color gradients within this corrected image to distinguish between regions of the scene. The first visual asset is aligned with these color gradients, ensuring a seamless integration into the environment.\n</p>\n<p>\n<strong>\n  Calculating Disparity Maps:\n </strong>\n The method involves calculating a disparity map by comparing the baseline image (from the camera\u2019s perspective) and the corrected color image (from the projector\u2019s perspective). This helps derive the three-dimensional representation of the scene.\n</p>\n<p>\n<strong>\n  Fusing Images for Depth Mapping:\n </strong>\n It also entails fusing the baseline and corrected color images to generate a depth map of the scene, providing further detail for accurate rendering.\n</p>\n<p>\n<strong>\n  Rendering and Warping Visual Assets:\n </strong>\n After transforming the baseline image, the corrected color image can be rendered in a creator window on a computing device. A second visual asset is aligned to another region of the scene, warped according to the three-dimensional profile of that region, and projected onto it.\n</p>\n<p>\n<strong>\n  Types of Visual Assets:\n </strong>\n The first visual asset may be a background image projected onto a background surface, while the second visual asset could include animations projected onto discrete objects in the scene, all from the projector's perspective.\n</p>\n<p>\n<strong>\n  Controller Functionality:\n </strong>\n The sequence of setup frames is served to the light projector through a controller that is connected to both the projector and the camera. The camera captures scan images in response to the setup frames being projected.\n</p>\n<p>\n<strong>\n  Horizontal Setup Frames:\n </strong>\n In one variation, the setup frames consist of alternating columns of black and white pixels. The corresponding scan images are captured during the projection of these frames, and the pixel correspondence map is calculated by converting these scan images into black-and-white formats.\n</p>\n<p>\n This detailed method provides a comprehensive approach to accurately overlaying augmented reality visual assets onto physical scenes, adapting to changes in depth, color, and\n</p>\n"}, "Page 24": {"response": "<h3>\n US 2020/0402247 A1: Enhanced Methods for Augmented Reality Projection\n</h3>\n<p>\n<strong>\n  Compiling Scan Images:\n </strong>\n The method includes compiling a set of black-and-white horizontal scan images into a composite scan image. This composite image is made up of an array of pixels, with each pixel having a specific address that corresponds to a pixel in the camera. Each pixel also contains a value that relates to a column of pixels in the light projector, intersecting at a surface in the scene.\n</p>\n<p>\n<strong>\n  Generating Setup Frames:\n </strong>\n</p>\n<ol>\n<li>\n<strong>\n   First Setup Frame:\n  </strong>\n  A first horizontal setup frame is created, consisting of a pixel array where:\n  <ul>\n<li>\n    Pixels with a value of \"0\" are assigned a black color.\n   </li>\n<li>\n    Pixels with a value of \"1\" are assigned a white color.\n   </li>\n</ul>\n</li>\n<li>\n<strong>\n   Second Setup Frame:\n  </strong>\n  Similarly, a second horizontal setup frame is generated, following the same pixel value assignments.\n </li>\n</ol>\n<p>\n<strong>\n  Capturing Scan Images:\n </strong>\n The method captures:\n</p>\n<ul>\n<li>\n  A first horizontal scan image when the first setup frame is projected onto the scene.\n </li>\n<li>\n  A second horizontal scan image during the projection of the second setup frame.\n </li>\n</ul>\n<p>\n<strong>\n  Calculating the Composite Image:\n </strong>\n The horizontal composite scan image is calculated by summing the first and second black-and-white scan images, with each image weighted according to its respective pixel values. The pixel values from this composite image are then used to create a pixel correspondence map, allowing for precise tracking of visual assets in the scene.\n</p>\n<p>\n<strong>\n  Transforming Baseline Images:\n </strong>\n Further steps include:\n</p>\n<ul>\n<li>\n  Capturing a baseline image of the scene through the camera's field of view.\n </li>\n<li>\n  Transforming this baseline image into a corrected color image based on the pixel correspondence map, which represents the scene from the light projector's perspective.\n </li>\n<li>\n  Detecting and highlighting objects within the corrected color image in a creator window.\n </li>\n</ul>\n<p>\n<strong>\n  Linking Visual Assets:\n </strong>\n Based on user selections, the method defines a specific region of the scene, delineated by the perimeter of a selected object, and links the corresponding visual asset to this region. The asset is warped to fit the profile of the object as represented in the three-dimensional model of the scene.\n</p>\n<p>\n<strong>\n  Projecting Visual Assets:\n </strong>\n The first visual asset is then projected onto the identified object in the scene using the light projector.\n</p>\n<p>\n<strong>\n  Setup and Operating Periods:\n </strong>\n The sequence of setup frames is served during a designated setup period, during which a baseline image is also captured. After this setup phase, the first visual asset is projected onto the defined region of the scene in a subsequent operating period.\n</p>\n<p>\n<strong>\n  Verification and Warping:\n </strong>\n In a following operating period, the method captures a verification image of the scene. A warp is calculated to align features from the baseline image with those in the verification image. If the warp exceeds a certain threshold, it is transformed to match the projector's domain, further warping the visual asset before projecting it onto the scene.\n</p>\n<p>\n This detailed approach enhances the accuracy and effectiveness of projecting augmented reality content onto dynamic scenes, ensuring that visual assets align correctly with real-world objects.\n</p>\n"}, "Page 25": {"response": "<h3>\n US 2020/0402247 A1: Methods for Dynamic Augmented Reality Projection\n</h3>\n<p>\n<strong>\n  Warping and Projecting Visual Assets:\n </strong>\n The method initiates when the warp's scope exceeds a predetermined threshold. At this point, a sequence of setup frames is served to the light projector, and a second set of scan images of the scene is captured. Using these images, a new pixel correspondence map is calculated, enabling the derivation of a second three-dimensional representation of the scene from the projector's perspective.\n</p>\n<p>\n Next, the process involves identifying distinct regions within this representation:\n</p>\n<ul>\n<li>\n<strong>\n   First Region:\n  </strong>\n  The primary area where the first content will be applied.\n </li>\n<li>\n<strong>\n   Second Region:\n  </strong>\n  An area separate from the first, set for different content.\n </li>\n</ul>\n<p>\n The first visual asset is then warped to fit the characteristics of the first region as defined in the new three-dimensional model and projected onto it using the light projector.\n</p>\n<p>\n<strong>\n  Content Application Mapping:\n </strong>\n Additionally, an asset map can be generated to link specific content applications to various regions within the scene. When accessing the first visual asset, it is retrieved from the designated content application assigned to the first region. Similarly, a second visual asset is accessed from another content application assigned to the second region. This second visual asset is warped according to the profile of its designated region and projected in response to a triggering event.\n</p>\n<p>\n<strong>\n  Motion Triggers and Dynamic Transitions:\n </strong>\n A motion trigger can be assigned to the second content application. While the first visual asset is projected, the system monitors for motion near the scene. Upon detecting such motion, it transitions from displaying the first visual asset to projecting the second visual asset in the corresponding second region.\n</p>\n<p>\n<strong>\n  Augmenting the Scene:\n </strong>\n To augment the scene, the method involves:\n</p>\n<ol>\n<li>\n  Serving a sequence of setup frames to the light projector.\n </li>\n<li>\n  Capturing scan images while each setup frame is being projected.\n </li>\n<li>\n  Creating a pixel correspondence map from these scan images and their corresponding setup frames.\n </li>\n<li>\n  Assigning content applications to different regions in the scene.\n </li>\n<li>\n  Warping the visual assets according to their respective region profiles and projecting them at designated times based on detected triggers.\n </li>\n</ol>\n<p>\n<strong>\n  Identifying Regions:\n </strong>\n The method can also utilize depth discontinuities within the three-dimensional representation to distinguish between different regions of the scene effectively.\n</p>\n<p>\n This comprehensive approach enables dynamic and responsive projection of augmented content, enhancing the interaction between virtual elements and the real-world environment.\n</p>\n"}, "Page 26": {"response": "<h3>\n US 2020/0402247 A1: Dynamic Projection of Visual Assets\n</h3>\n<p>\n<strong>\n  Warping and Projecting Visual Assets:\n </strong>\n The process involves two visual assets related to specific regions within a three-dimensional scene representation.\n</p>\n<ol>\n<li>\n<p>\n<strong>\n    First Visual Asset:\n   </strong>\n</p>\n<ul>\n<li>\n    A visual asset is warped to fit the profile of the first region in the scene.\n   </li>\n<li>\n    During an initial time period, this first visual asset is projected onto the first region using a light projector.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Second Visual Asset:\n   </strong>\n</p>\n<ul>\n<li>\n    A second visual asset, derived from a different content application, is also warped to align with the profile of the first region.\n   </li>\n<li>\n    Following the first projection, during a subsequent time period, the second visual asset is projected onto the second region of the scene. This projection occurs in response to detecting a specific trigger event, ensuring a dynamic interaction with the environment.\n   </li>\n</ul>\n</li>\n</ol>\n<p>\n This method allows for a seamless transition between visual assets, enhancing the augmented reality experience by adapting projections to real-time stimuli.\n</p>\n\nChat Question:\nExtensive summary, what is the whole passage about?\n\n<p>\n The passage outlines a method for enhancing real-world scenes using projected visual content through a light projector. It describes a series of steps and processes involved in accurately aligning and displaying these visual assets based on a three-dimensional representation of the scene.\n</p>\n<h3>\n Key Components:\n</h3>\n<ol>\n<li>\n<p>\n<strong>\n    Setup Process:\n   </strong>\n</p>\n<ul>\n<li>\n    The method begins with serving a sequence of setup frames to the light projector, which helps prepare the scene for projection.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Image Capture:\n   </strong>\n</p>\n<ul>\n<li>\n    A camera captures a set of scan images while the setup frames are being projected. This ensures that the images reflect the current state of the scene, enabling the system to gather accurate data.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Pixel Correspondence Mapping:\n   </strong>\n</p>\n<ul>\n<li>\n    The captured scan images are analyzed alongside the setup frames to create a pixel correspondence map. This map is essential for understanding how pixels in the captured images relate to the physical scene.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Three-Dimensional Scene Representation:\n   </strong>\n</p>\n<ul>\n<li>\n    Using the pixel correspondence map, a three-dimensional representation of the scene is derived. This representation is crucial for identifying distinct regions within the scene.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Region Identification:\n   </strong>\n</p>\n<ul>\n<li>\n    The method identifies specific regions within the scene, allowing for targeted content application. For instance, it distinguishes between different areas based on features such as depth discontinuity.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Visual Asset Warping:\n   </strong>\n</p>\n<ul>\n<li>\n    Visual assets are warped to fit the profiles of the identified regions. This warping ensures that the projections align correctly with the surfaces they are intended to enhance.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Dynamic Projection:\n   </strong>\n</p>\n<ul>\n<li>\n    The method includes projecting the first visual asset onto the first identified region, followed by projecting a second visual asset onto a different region in response to a trigger event. This could be a change in the environment or user interaction, promoting a responsive and engaging augmented reality experience.\n   </li>\n</ul>\n</li>\n<li>\n<p>\n<strong>\n    Content Application:\n   </strong>\n</p>\n<ul>\n<li>\n    The system can manage multiple content applications for different regions, facilitating the projection of various visual assets based on the context or user actions.\n   </li>\n</ul>\n</li>\n</ol>\n<h3>\n Conclusion:\n</h3>\n<p>\n Overall, the passage describes a sophisticated approach to augmenting real-world environments through projective technology, emphasizing the importance of accuracy in image mapping, region identification, and dynamic interactions. This method has potential applications in areas such as entertainment, education, and interactive displays, where merging digital content with physical spaces can create immersive experiences.\n</p>\n"}}